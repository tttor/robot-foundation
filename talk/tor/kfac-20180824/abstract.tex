\section{Abstract}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Abstract}

Problem:
\begin{itemize}
    \item for natural gradients, there is still NO way to efficiently compute
        the inverse of Fisher info matrix $F^{-1}$ (or its product with a vector)
\end{itemize}

Idea:
\begin{itemize}
    \item approximate $F^{-1}$ as block diagonal or block tridiagonal matrices
\end{itemize}

Result:
\begin{itemize}
    \item much faster (fewer iterations) than well-tuned SGD with momentum
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Abstract}
{\footnotesize
Left to right: \\
$\tilde{F}^{-1}$, its approximations $\breve{F}^{-1}$ (top) and $\hat{F}^{-1}$ (bottom), their absolute difference
}
\begin{figure}
    \centering
    \includegraphics[scale=0.2]{kfac_12}
\end{figure}
(plotting absolute values of entries, dark means small)
\end{frame}

\begin{frame}
\frametitle{Abstract}
\begin{figure}
    \centering
    \includegraphics[scale=0.25]{mnist_autoencoder}
\end{figure}
(Baseline: well-tuned SGD with momentum)
\end{frame}
