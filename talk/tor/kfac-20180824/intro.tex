\section{Introduction}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Introduction}

\begin{figure}
    \centering
    \includegraphics[scale=0.2]{net}
\end{figure}

\begin{itemize}
    \item $\bar{a}_i$ is $a_i$ appended by a homogeneous coordinate with value 1, ie
            to capture bias parameters explicitly
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Introduction}

\begin{figure}
    \centering
    \includegraphics[scale=0.2]{net_param}
\end{figure}

\begin{itemize}
    \item $vec$ vectorizes matrices by stacking their columns together
    \item $\hat{Q}_{x, y}$ is a training distribution
    \item $p(y|x, \theta) = r(y|f(x, \theta))$ is the density function of $P_{y|x}(\theta) = R_{y|f(x,\theta)}$
    \item minimizing $h(\theta)$ can be seen as maximum likelihood learning of~$P_{y|x}(\theta)$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Introduction}
\begin{figure}
    \centering
    \includegraphics[scale=0.25]{natgrad}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{fisher}
\end{figure}

\end{frame}
